{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning with Python\n",
    "## Chapter 2. 지도학습\n",
    "---\n",
    "## 경사 하강법 (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법은 예측함수를 만들고, 훈련데이터에 대한 예측결과 오류를 점점 줄여나가는 방법이다.\n",
    "- 선형회귀에서 속성이 하나일 때, 즉 키가 입력값이고 몸무계가 목표값일 때를 예로 들자.\n",
    "> $ \\hat{y} = w x + b $\n",
    "\n",
    "- 이 경우, 목표값인 $ y $ 에 예측값인 $ \\hat{y} $ 가 최대한 근접하도록 $w$ 와 $b$ 값을 구해야 한다.\n",
    "- $y$ 와 $\\hat{y}$ 의 오류를 수치화한 값이 앞에서 배운 MSE(Mean Squared Error) 이다.\n",
    "> $ MSE = {1 \\over N} \\sum_{i=0}^{N-1} (y_i - \\hat{y}_i)^2 $ \n",
    "$ = {1 \\over N} \\sum_{i=0}^{N-1} (y_i - (w x_i + b))^2 $\n",
    "\n",
    "- 경사하강법의 목표는 오류(비용 또는 손실) 값인 MSE 를 최소화하는 w, b 값을 찾는 것이다. 그런데 위의 수식을 보면 x 와 y 의 값은 데이터에서 주어진 상수이므로, 오류는 w 와 b 의 2차함수임을 알 수 있다.\n",
    "> 오류를 나타내는 용어는 여러가지이다. 오류(error), 비용(cost), 손실(loss) 등이다. 이를 비용함수(cost function) 이라는 용어로도 사용하는데, 이것은 w 와 b 의 값을 인자로 보기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ch02_15.jpg\" />\n",
    "<p style=\"text-align: center;\">(출처: https://hackernoon.com/gradient-descent-aynk-7cbe95a778da)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림을 보며 설명해 보자. 그림에서 J(w) 는 MSE 를 의미한다. 다른 말로는 비용함수 라고도 한다.\n",
    "- w 값에 따라 비용함수의 값이 변하는데, 최종적으로 제일 아래부분에 도달하는 w 값을 찾아야 한다.\n",
    "- 경사하강법에서는 w 와 b 에 대해 초기값을 지정하고 시작한다. 예를 들어 w=5, b=1 과 같다. 까만 점이 w=5 라고 가정하면 그 부분에서의 **기울기값 만큼 반대방향** 으로 w 값을 바꾸어주면 된다.\n",
    "- 이렇게 단계적으로 w 값을 바꿔가면서 비용값을 최소화 시키는 방법이 경사를 따라 내려가는 것과 비슷하다고 해서 경사하강법이란 이름으로 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림에서 기울기 값은 아래와 같이 구한다.\n",
    "> $ {\\partial MSE \\over \\partial w} = -{2 \\over N} \\sum_{i=0}^{N-1} (y_i - \\hat{y}_i) x_{i} $ <br>\n",
    "\n",
    "- 기울기 만큼 w 값을 줄여나가야 하므로 새로운 w 값은 아래와 같다.\n",
    "> $ w \\Rightarrow w - \\eta {\\partial MSE \\over \\partial w} = w + {2 \\over N} \\eta \\sum_{i=0}^{N-1} (y_i - \\hat{y}_i) x_{i} $ <br>\n",
    "\n",
    "- 여기서 $\\eta$ 는 학습률(learning rate)로서 한번에 얼마만큼 w 값을 바꿔나갈 지를 지정한 값이다. 보통은 아주 작은 값으로 지정한다.\n",
    "\n",
    "- 이런 과정을 여러번 반복하면 결국 비용함수를 최소화하는 w 값을 근사적으로 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 선형회귀를 예로 들었지만,  sigmoid 함수로 예측함수를 지정하면 로지스틱 회귀가 된다.\n",
    "> $ \\hat{y_i} = {1 \\over {1 + e^{-(w x_i + b)}}} $\n",
    "\n",
    "- 속성이 하나가 아니라 m 개일 때로 일반화 하면 아래와 같다. (속성계수인 w 값도 m 개가 된다.)\n",
    "> $ \\hat{y_i} = \\sum_{j=0}^{m-1} w_j x_{ij} + b $ <br>\n",
    "or<br>\n",
    "> $ \\hat{y_i} = \\sum_{j=0}^{m-1} f(w_j x_{ij} + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법은 **신경망**과 **딥러닝**의 핵심 알고리즘이다. 딥러닝을 이해하려면 경사하강법에 대한 이해는 필수이다.\n",
    "- sklearn.linear_model 에 있는 SGDRegressor 와 SGDClassifier 를 활용할 수 있다. 이들은 기존 선형 알고리즘과 결과는 동일하며 단지 내부 알고리즘에 차이가 있을 뿐이다.\n",
    "- 경사하강법은 w 값을 변경하는 시기와 관련되어 아래 세가지로 구분한다.\n",
    "> 1. **배치(Batch)** 경사하강법 : 샘플을 한번에 모두 넣어 w 값을 한번 바꾼다. 이를 epoch(시대) 수 만큼 반복한다.\n",
    "> 2. **확률적(Stochastic)** 경사하강법 : 샘플 하나를 적용할 때 마다 w 값을 바꾼다.\n",
    "> 3. **미니배치(Mini batch)** 경사하강법 : 샘플을 여러 세트로 나누어 한 세트를 적용할 때 마다 w 값을 바꾼다. 모든 세트가 끝나면, epoch 만큼 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [참고] 경사하강법의 일반해\n",
    "- 아래 수식을 활용하면 Numpy 를 사용하여 선형회귀나 로지스틱회귀를 구현할 수 있다.\n",
    "\n",
    "$ \\hat{y_i} = f(\\sum_{j=0}^{m-1} w_j x_{ij} + b) $ <br>\n",
    "$ Loss = {1 \\over N} \\sum_{i=0}^{N-1} (y_i - \\hat{y}_i)^2 $ <br>\n",
    "$ {\\partial Loss \\over \\partial w_j} = -{2 \\over N} \\sum_{i=0}^{N-1} [(y_i - \\hat{y}_i)\n",
    "\\cdot f^\\prime \\cdot x_{ij}] $ <br>\n",
    "$ \\Delta w_j = - \\eta {\\partial Loss \\over \\partial w_j}\n",
    "    = \\eta {2 \\over N} \\sum_{i=0}^{N-1} [(y_i - \\hat{y}_i) \\cdot f^\\prime \\cdot x_{ij}] $ <br>\n",
    "$ \\Delta b = - \\eta {\\partial Loss \\over \\partial b}\n",
    "    = \\eta {2 \\over N} \\sum_{i=0}^{N-1} [(y_i - \\hat{y}_i) \\cdot f^\\prime] $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
